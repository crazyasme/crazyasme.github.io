<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Chapter 1</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>

<body class="is-preload">

    <!-- Wrapper -->
    <div id="wrapper">

        <!-- Nav -->
        <nav id="nav">
            <ul class="links">
                <li class="active"><a href="index.html">文章</a></li>
            </ul>
            <ul class="icons">
                <li><a href="https://twitter.com/TooCrazyAsMe" class="icon brands fa-twitter"><span
                            class="label">Twitter</span></a></li>
                <li><a href="https://github.com/crazyasme" class="icon brands fa-github"><span
                            class="label">GitHub</span></a></li>
            </ul>
        </nav>

        <!-- Main -->
        <div id="main">

            <!-- Post -->
            <section class="post">
                <header class="major">
                    <h1>线性回归——最小二乘法</h1>
                </header>

                <p>
                    如果有数据
                    <div id = "overflow">
                        \[X = \begin{pmatrix} x_1 & x_2 & x_3 & \cdots & x_N\end{pmatrix}^T = \begin{pmatrix}x_1^T\\x_2^T\\x_3^T\\\vdots\\x_N^T\end{pmatrix} = \begin{pmatrix} x_{1,1}&x_{1,2}&x_{1,3}&\cdots&x_{1,p}\\x_{2,1}&x_{2,2}&x_{2,3}&\cdots&x_{2,p}\\x_{3,1}&x_{3,2}&x_{3,3}&\cdots&x_{3,p}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\x_{N,1}&x_{N,2}&x_{N,3}&\cdots&x_{N,p}\\\end{pmatrix}\]
                    </div>
                    与对应的值
                    \[Y = \begin{pmatrix}y_1&y_2&y_3&\cdots&y_N\end{pmatrix}^T\tag{1}\]
                    对这组数据进行线性拟合，即找到一条直线
                    \[F(w) = w^Tx \tag{2}\]
                    最小二乘估计（least square estimation）是最简单的模型之一。其损失函数为欧几里德范数求和
                    \[L(w) = \sum_{i=1}^N ||w^Tx_i-y_i||_2^2 \tag{3}\]
                    下面我们尝试将其改写为矩阵形式
                    <div id="overflow">
                        \[\begin{aligned}L(w) &=\sum_{i=1}^N(w^Tx_i-y_i)^2\\&=\begin{pmatrix}w^Tx_1-y_1&w^Tx_2-y_2&w^Tx_3-y_3&\cdots&w^Tx_N-y_N\end{pmatrix}\begin{pmatrix}w^Tx_1-y_1\\w^Tx_2-y_2\\w^Tx_3-y_3\\\vdots\\w^Tx_N-y_N\end{pmatrix} \\&=(W^TX^T-Y^T)(XW-Y)\\&=W^TX^TXW-2W^TX^TY+Y^TY\end{aligned}\]
                    </div>
                    我们要求的\(\hat W\)，即
                    \[\hat W = arg\underset W minL(W)\tag{4}\]
                    自然，我们要对\(w\)求导
                    \[\frac{\partial L(W)}{\partial W} = 2X^TXW-2X^TY\]
                    这就是W的解析解。\[{(X^TX)^{-1}X^T}\]被称作伪逆。<br><br>
                    若要了解最小二乘法的几何意义，我们需要回到开头提到的数据矩阵
                    <div id = "overflow">
                        \[X = \begin{pmatrix} x_1 & x_2 & x_3 & \cdots & x_N\end{pmatrix}^T = \begin{pmatrix}x_1^T\\x_2^T\\x_3^T\\\vdots\\x_N^T\end{pmatrix} = \begin{pmatrix} x_{1,1}&x_{1,2}&x_{1,3}&\cdots&x_{1,p}\\x_{2,1}&x_{2,2}&x_{2,3}&\cdots&x_{2,p}\\x_{3,1}&x_{3,2}&x_{3,3}&\cdots&x_{3,p}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\x_{N,1}&x_{N,2}&x_{N,3}&\cdots&x_{N,p}\\\end{pmatrix}\]
                    </div>
                    若改变审视这个矩阵的方式，我们发现，我们看到了一个由一组列向量张成的\(p\)维子空间，不妨称为\(\alpha\)。而通常情况下，\(Y\)不会在\(\alpha\)中（如果这么巧的话，我们会很开心的）。我们做的线性拟合，就是做一条直线，使得\(Y\)与\(\alpha\)的距离最近。
                    那么改写\((2)\)式
                    \[f(w)=w^Tx=x^T\beta \tag{6}\]
                    容易看出法向量\(Y-X^T\beta\)。因为\(Y-X^T\beta\)为\(\alpha\)的法向量，所以有
                    \[X^T(Y-X\beta)=0\]
                    解得
                    \[\beta={(X^T)X}^{-1}X^TY \tag{7}\]
                    我们发现，\((7)\)式与\((5)\)式殊途同归。<br><br>
                    也可以从另一个角度观察最小二乘法的几何意义，由\((3)\)式可知，最小二乘法即将损失分配到每个样本（而前文说的角度则是将损失分配到每个维度）。<br><br>
                    我们还可以从概率角度理解最小二乘法。下面将从概率角度对最小二乘法进行推导。<br><br>
                    我们知道，现实情况下，\(Y\)很难与\(X\)完美拟合，经常会有噪声，我们记作\(\epsilon\)。我们需要假设\(\epsilon\)服从高斯分布，这会在接下来的演算里体现
                    \[\epsilon \sim N(0,\sigma^2)\]
                    我们有
                    \[y = f(w) + \epsilon\]
                    其中\(f(w) = w^Tx+b\)。我们将  \(b\) 称为偏置项，此处我们可以将其省略。于是
                    \[y=w^Tx+\epsilon\]
                    显然
                    \[y|x;w \sim N(w^Tx,\sigma^2）\\P(y|x;w) = \frac{1}{\sqrt{2\pi}\sigma}exp\{\frac{{(y-w^Tx)}^2}{2\sigma^2}\} \tag{9}\]
                    接下来可以使用MLE
                    \[\begin{aligned}L(w) &= lnP(Y|X;w)\\ &= ln\prod_{i=1}^NP(y_i|x_i;w)\\ &= \sum_{i=1}{N}lnP(y_i|x_i;w)\\ &=\sum_{i=1}^N(ln\frac{1}{\sqrt{2\pi}\sigma}-\frac{{(y-w^Tx)}^2}{2\sigma^2})\end{aligned}\]
                    \[\begin{aligned}\hat W_{MLE} &= arg\underset W max L(w)\\&=arg\underset W max\sum_{i=1}^{N}-\frac{{(y_i-w^Tx_i)}^2}{2\sigma^2}\\&=arg\underset W min\sum_{i=1}^{N}{(y_i-w^Tx_i)}^2\end{aligned}\tag{10}\]
                    而\((10)\)式中的\(\sum_{i=1}^{N}{(y_i-w^Tx_i)}^2\)即为欧几里德范数，与\((3)\)式相同。

                </p>

                <ul class="actions">
                    <li>
                        <a href="index.html" class="button">回到首页</a>
                    </li>
                </ul>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		
			<style>
				li{
					left: 0;
					right: 0;
					margin: auto
				}

                #overflow{
                    overflow: auto;
                }
			</style>
	</body>
</html>